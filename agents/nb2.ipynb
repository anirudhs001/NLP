{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: user: Hello, how are you?\\nassistant:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOllama] [2.39s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"Hello! I'm just a computer program, so I don't have feelings like humans do. But thank you for asking! How can I assist you today?\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1:8b\",\n",
      "          \"created_at\": \"2025-03-08T06:47:16.764367Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 2390208083,\n",
      "          \"load_duration\": 35928791,\n",
      "          \"prompt_eval_count\": 20,\n",
      "          \"prompt_eval_duration\": 1720000000,\n",
      "          \"eval_count\": 33,\n",
      "          \"eval_duration\": 631000000,\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\",\n",
      "            \"images\": null,\n",
      "            \"tool_calls\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"Hello! I'm just a computer program, so I don't have feelings like humans do. But thank you for asking! How can I assist you today?\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1:8b\",\n",
      "              \"created_at\": \"2025-03-08T06:47:16.764367Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 2390208083,\n",
      "              \"load_duration\": 35928791,\n",
      "              \"prompt_eval_count\": 20,\n",
      "              \"prompt_eval_duration\": 1720000000,\n",
      "              \"eval_count\": 33,\n",
      "              \"eval_duration\": 631000000,\n",
      "              \"message\": {\n",
      "                \"lc\": 1,\n",
      "                \"type\": \"not_implemented\",\n",
      "                \"id\": [\n",
      "                  \"ollama\",\n",
      "                  \"_types\",\n",
      "                  \"Message\"\n",
      "                ],\n",
      "                \"repr\": \"Message(role='assistant', content='', images=None, tool_calls=None)\"\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-a74b70dd-eb3b-442c-9653-79409f2bb9d6-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 20,\n",
      "              \"output_tokens\": 33,\n",
      "              \"total_tokens\": 53\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "content=\"Hello! I'm just a computer program, so I don't have feelings like humans do. But thank you for asking! How can I assist you today?\" additional_kwargs={} response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-03-08T06:47:16.764367Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2390208083, 'load_duration': 35928791, 'prompt_eval_count': 20, 'prompt_eval_duration': 1720000000, 'eval_count': 33, 'eval_duration': 631000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-a74b70dd-eb3b-442c-9653-79409f2bb9d6-0' usage_metadata={'input_tokens': 20, 'output_tokens': 33, 'total_tokens': 53}\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "\n",
    "res = llm.invoke(\"user: Hello, how are you?\\nassistant:\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, ChatMessage, HumanMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.tools import tool\n",
    "import langchain\n",
    "langchain.debug=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "typing.Annotated[list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x10e21f6a0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'twinkle twinkle']\n",
      "('twinkle twinkle',)\n",
      "('twinkle twinkle',)\n"
     ]
    }
   ],
   "source": [
    "from typing import get_type_hints\n",
    "class DummyState(TypedDict):\n",
    "    # first arg to annotated: a valid type\n",
    "    # second arg: any metadata. here it's a function\n",
    "    messages: Annotated[list[AnyMessage], \"twinkle twinkle\"]\n",
    "\n",
    "# ways to get the annotation out of AgentState:\n",
    "# 1. get_type_hints\n",
    "state = DummyState(messages=[])\n",
    "type_hints = get_type_hints(DummyState, include_extras=True)\n",
    "print(type_hints[\"messages\"])\n",
    "print(type_hints[\"messages\"].__metadata__)\n",
    "\n",
    "# directly from the class\n",
    "print(DummyState.__annotations__[\"messages\"].__metadata__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function AgentState.<lambda> at 0x11b0945e0>\n",
      "<class 'str'>\n",
      "                        lambda x, y: x + y]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage],\n",
    "                        lambda x, y: x + y]\n",
    "\n",
    "func = AgentState.__annotations__[\"messages\"].__metadata__[0]\n",
    "print(func)\n",
    "import inspect\n",
    "print(type(inspect.getsource(func)))\n",
    "print(inspect.getsource(func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model: BaseChatModel, tools, system_msg=\"\"):\n",
    "        self.system_msg = system_msg\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_model)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", \n",
    "            self.action_exists,\n",
    "            {\n",
    "                True: \"action\",\n",
    "                False: END\n",
    "            }\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {\n",
    "            t.name: t\n",
    "            for t in tools\n",
    "        }\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "    def call_model(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system_msg:\n",
    "            messages = [SystemMessage(content=self.system_msg)] + messages\n",
    "        out = self.model.invoke(messages)\n",
    "        return {\"messages\": [out]}\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for tool_call in tool_calls:\n",
    "            print(f\"Calling tool {tool_call['name']} with args {tool_call['args']}\")\n",
    "            if tool_call['name'] not in self.tools:\n",
    "                print(f\"Tool {tool_call['name']} not found\")\n",
    "                result = \"tool doesn't exist, retry\"\n",
    "            else:\n",
    "                result = self.tools[tool_call['name']].invoke(tool_call['args'])\n",
    "            print(f\"Tool {tool_call['name']}({tool_call['args']}) returned {result}\")\n",
    "            results.append(ToolMessage(tool_call_id=tool_call['id'], content=result, name=tool_call['name']))\n",
    "        return {\"messages\": results}\n",
    "\n",
    "    def action_exists(self, state: AgentState):\n",
    "        last_msg = state[\"messages\"][-1]\n",
    "        return len(last_msg.tool_calls) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def multiply_with_decorator(a, b):\n",
    "    \"\"\"\n",
    "    Multiply two numbers\n",
    "    Args:\n",
    "        a: The first number to multiply\n",
    "        b: The second number to multiply\n",
    "    Returns:\n",
    "        The product of the two numbers\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "class MultiplyTool():\n",
    "    def __init__(self):\n",
    "        self.name = \"multiply\"\n",
    "        self.description = \"Multiply two numbers\"\n",
    "    \n",
    "    def _run(self, a, b):\n",
    "        return a * b\n",
    "    \n",
    "class AsyncMultiplyTool():\n",
    "    def __init__(self):\n",
    "        self.name = \"multiply\"\n",
    "        self.description = \"Multiply two numbers\"\n",
    "    \n",
    "    async def _arun(self, a, b):\n",
    "        return a * b\n",
    "\n",
    "def divide_tool(a, b):\n",
    "    return a / b\n",
    "\n",
    "# from langchain_core.tools import tool\n",
    "# multiply_tool = MultiplyTool()\n",
    "# multiply_tool.invoke(1, 2)\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools([MultiplyTool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What is 2 times 3?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOllama] [2.76s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1:8b\",\n",
      "          \"created_at\": \"2025-03-08T08:48:28.62776Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 2760215791,\n",
      "          \"load_duration\": 32940041,\n",
      "          \"prompt_eval_count\": 147,\n",
      "          \"prompt_eval_duration\": 2301000000,\n",
      "          \"eval_count\": 23,\n",
      "          \"eval_duration\": 422000000,\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\",\n",
      "            \"images\": null,\n",
      "            \"tool_calls\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1:8b\",\n",
      "              \"created_at\": \"2025-03-08T08:48:28.62776Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 2760215791,\n",
      "              \"load_duration\": 32940041,\n",
      "              \"prompt_eval_count\": 147,\n",
      "              \"prompt_eval_duration\": 2301000000,\n",
      "              \"eval_count\": 23,\n",
      "              \"eval_duration\": 422000000,\n",
      "              \"message\": {\n",
      "                \"lc\": 1,\n",
      "                \"type\": \"not_implemented\",\n",
      "                \"id\": [\n",
      "                  \"ollama\",\n",
      "                  \"_types\",\n",
      "                  \"Message\"\n",
      "                ],\n",
      "                \"repr\": \"Message(role='assistant', content='', images=None, tool_calls=None)\"\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f50fe6af-7c15-429a-81f5-b12402cbecde-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"MultiplyTool\",\n",
      "                \"args\": {\n",
      "                  \"a\": 2,\n",
      "                  \"b\": 3\n",
      "                },\n",
      "                \"id\": \"f96afd31-2e22-4993-a5bd-95af8905a197\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 147,\n",
      "              \"output_tokens\": 23,\n",
      "              \"total_tokens\": 170\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "out = llm_with_tools.invoke(\"What is 2 times 3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'MultiplyTool', 'args': {'a': 2, 'b': 3}, 'id': 'f96afd31-2e22-4993-a5bd-95af8905a197', 'type': 'tool_call'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint, PrettyPrinter\n",
    "print(out.tool_calls)\n",
    "print(out.content)\n",
    "\n",
    "# TODO: figure out how to see the internal prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: What info do we have in ecommerce_db.users table?\"\n",
      "  ]\n",
      "}\n",
      "{'additional_kwargs': {},\n",
      " 'content': '',\n",
      " 'example': False,\n",
      " 'id': 'run-b0c3669b-1b37-48f9-b108-4385614e0cb3',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {},\n",
      " 'tool_call_chunks': [{'args': '{\"db\": \"ecommerce_db\", \"query\": \"SELECT * FROM '\n",
      "                               'users\"}',\n",
      "                       'id': 'b89d5cc9-280f-4b52-b1a6-54873643b62e',\n",
      "                       'index': None,\n",
      "                       'name': 'sql',\n",
      "                       'type': 'tool_call_chunk'}],\n",
      " 'tool_calls': [{'args': {'db': 'ecommerce_db', 'query': 'SELECT * FROM users'},\n",
      "                 'id': 'b89d5cc9-280f-4b52-b1a6-54873643b62e',\n",
      "                 'name': 'sql',\n",
      "                 'type': 'tool_call'}],\n",
      " 'type': 'AIMessageChunk',\n",
      " 'usage_metadata': None}\n",
      "{'additional_kwargs': {},\n",
      " 'content': '',\n",
      " 'example': False,\n",
      " 'id': 'run-b0c3669b-1b37-48f9-b108-4385614e0cb3',\n",
      " 'invalid_tool_calls': [],\n",
      " 'name': None,\n",
      " 'response_metadata': {'created_at': '2025-03-08T08:48:29.249049Z',\n",
      "                       'done': True,\n",
      "                       'done_reason': 'stop',\n",
      "                       'eval_count': 27,\n",
      "                       'eval_duration': 486000000,\n",
      "                       'load_duration': 12172083,\n",
      "                       'message': Message(role='assistant', content='', images=None, tool_calls=None),\n",
      "                       'model': 'llama3.1:8b',\n",
      "                       'prompt_eval_count': 150,\n",
      "                       'prompt_eval_duration': 90000000,\n",
      "                       'total_duration': 589640708},\n",
      " 'tool_call_chunks': [],\n",
      " 'tool_calls': [],\n",
      " 'type': 'AIMessageChunk',\n",
      " 'usage_metadata': {'input_tokens': 150,\n",
      "                    'output_tokens': 27,\n",
      "                    'total_tokens': 177}}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[llm:ChatOllama] [592ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1:8b\",\n",
      "          \"created_at\": \"2025-03-08T08:48:29.249049Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 589640708,\n",
      "          \"load_duration\": 12172083,\n",
      "          \"prompt_eval_count\": 150,\n",
      "          \"prompt_eval_duration\": 90000000,\n",
      "          \"eval_count\": 27,\n",
      "          \"eval_duration\": 486000000,\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\",\n",
      "            \"images\": null,\n",
      "            \"tool_calls\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGenerationChunk\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessageChunk\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1:8b\",\n",
      "              \"created_at\": \"2025-03-08T08:48:29.249049Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 589640708,\n",
      "              \"load_duration\": 12172083,\n",
      "              \"prompt_eval_count\": 150,\n",
      "              \"prompt_eval_duration\": 90000000,\n",
      "              \"eval_count\": 27,\n",
      "              \"eval_duration\": 486000000,\n",
      "              \"message\": {\n",
      "                \"lc\": 1,\n",
      "                \"type\": \"not_implemented\",\n",
      "                \"id\": [\n",
      "                  \"ollama\",\n",
      "                  \"_types\",\n",
      "                  \"Message\"\n",
      "                ],\n",
      "                \"repr\": \"Message(role='assistant', content='', images=None, tool_calls=None)\"\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"AIMessageChunk\",\n",
      "            \"id\": \"run-b0c3669b-1b37-48f9-b108-4385614e0cb3\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"sql\",\n",
      "                \"args\": {\n",
      "                  \"db\": \"ecommerce_db\",\n",
      "                  \"query\": \"SELECT * FROM users\"\n",
      "                },\n",
      "                \"id\": \"b89d5cc9-280f-4b52-b1a6-54873643b62e\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 150,\n",
      "              \"output_tokens\": 27,\n",
      "              \"total_tokens\": 177\n",
      "            },\n",
      "            \"tool_call_chunks\": [\n",
      "              {\n",
      "                \"name\": \"sql\",\n",
      "                \"args\": \"{\\\"db\\\": \\\"ecommerce_db\\\", \\\"query\\\": \\\"SELECT * FROM users\\\"}\",\n",
      "                \"id\": \"b89d5cc9-280f-4b52-b1a6-54873643b62e\",\n",
      "                \"index\": null,\n",
      "                \"type\": \"tool_call_chunk\"\n",
      "              }\n",
      "            ],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "messages = [HumanMessage(content=\"What info do we have in ecommerce_db.users table?\")]\n",
    "\n",
    "for chunk in llm_with_tools.stream(messages):\n",
    "    pprint(vars(chunk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"\"\"You are a smart research assistant. \\\n",
    "Use the multiply_with_decorator tool to do multiplication. \\\n",
    "Use the divide_tool to do division. \\\n",
    "You can only make one tool call at a time. \\\n",
    "Do not call another tool until you have the output of the previous tool call. \\\n",
    "Example:\n",
    "what is 2 times 3 divided by 4?\n",
    "mutiply 2 and 3 first\n",
    "2 times 3 is 6\n",
    "\n",
    "2 times 3 divided by 4 is \n",
    "(2 times 3) divided by 4 which is\n",
    "6 divided by 4\n",
    "divide 6 by 4 to get 1.5\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.1:8b\", temperature=0)\n",
    "\n",
    "from langchain_core.tools import BaseTool\n",
    "class DivideTool(BaseTool):\n",
    "    name:str = \"divide\"\n",
    "    description:str = \"Divide two numbers\"\n",
    "    \n",
    "    def _run(self, a, b):\n",
    "        return a / b\n",
    "    \n",
    "divide_tool = DivideTool()\n",
    "agent = Agent(model, [multiply_with_decorator, divide_tool], prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "Image(agent.graph.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<...>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<...>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<start:llm>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__ > chain:ChannelWrite<start:llm>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:__start__] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a smart research assistant. Use the multiply_with_decorator tool to do multiplication. Use the divide_tool to do division. You can only make one tool call at a time. Do not call another tool until you have the output of the previous tool call. Example:\\nwhat is 2 times 3 divided by 4?\\nmutiply 2 and 3 first\\n2 times 3 is 6\\n\\n2 times 3 divided by 4 is \\n(2 times 3) divided by 4 which is\\n6 divided by 4\\ndivide 6 by 4 to get 1.5\\n\\nHuman: What is 11 times 142 divided by 8?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > llm:ChatOllama] [2.97s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1:8b\",\n",
      "          \"created_at\": \"2025-03-08T09:18:37.242426Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 2962564291,\n",
      "          \"load_duration\": 32911750,\n",
      "          \"prompt_eval_count\": 377,\n",
      "          \"prompt_eval_duration\": 2053000000,\n",
      "          \"eval_count\": 47,\n",
      "          \"eval_duration\": 873000000,\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"\",\n",
      "            \"images\": null,\n",
      "            \"tool_calls\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1:8b\",\n",
      "              \"created_at\": \"2025-03-08T09:18:37.242426Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 2962564291,\n",
      "              \"load_duration\": 32911750,\n",
      "              \"prompt_eval_count\": 377,\n",
      "              \"prompt_eval_duration\": 2053000000,\n",
      "              \"eval_count\": 47,\n",
      "              \"eval_duration\": 873000000,\n",
      "              \"message\": {\n",
      "                \"lc\": 1,\n",
      "                \"type\": \"not_implemented\",\n",
      "                \"id\": [\n",
      "                  \"ollama\",\n",
      "                  \"_types\",\n",
      "                  \"Message\"\n",
      "                ],\n",
      "                \"repr\": \"Message(role='assistant', content='', images=None, tool_calls=None)\"\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3cf8d302-0382-4e57-aba8-30320c2d73fc-0\",\n",
      "            \"tool_calls\": [\n",
      "              {\n",
      "                \"name\": \"multiply_with_decorator\",\n",
      "                \"args\": {\n",
      "                  \"a\": 11,\n",
      "                  \"b\": 142\n",
      "                },\n",
      "                \"id\": \"7d7f5423-9008-441c-b7d7-9de954126a6d\",\n",
      "                \"type\": \"tool_call\"\n",
      "              },\n",
      "              {\n",
      "                \"name\": \"divide\",\n",
      "                \"args\": {\n",
      "                  \"a\": 1562,\n",
      "                  \"b\": 8\n",
      "                },\n",
      "                \"id\": \"b6043626-4fa3-4549-9e6c-bb233e4aeb75\",\n",
      "                \"type\": \"tool_call\"\n",
      "              }\n",
      "            ],\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 377,\n",
      "              \"output_tokens\": 47,\n",
      "              \"total_tokens\": 424\n",
      "            },\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > chain:ChannelWrite<...,llm>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > chain:ChannelWrite<...,llm>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > chain:action_exists] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > chain:action_exists] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": true\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm] [2.97s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:action] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "Calling tool multiply_with_decorator with args {'a': 11, 'b': 142}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:action > tool:multiply_with_decorator] Entering Tool run with input:\n",
      "\u001b[0m\"{'a': 11, 'b': 142}\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:action > tool:multiply_with_decorator] [1ms] Exiting Tool run with output:\n",
      "\u001b[0m\"1562\"\n",
      "Tool multiply_with_decorator({'a': 11, 'b': 142}) returned 1562\n",
      "Calling tool divide with args {'a': 1562, 'b': 8}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:action > tool:divide] Entering Tool run with input:\n",
      "\u001b[0m\"{'a': 1562, 'b': 8}\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:action > tool:divide] [0ms] Exiting Tool run with output:\n",
      "\u001b[0m\"195.25\"\n",
      "Tool divide({'a': 1562, 'b': 8}) returned 195.25\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:action > chain:ChannelWrite<...,action>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:action > chain:ChannelWrite<...,action>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:action] [1ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > llm:ChatOllama] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are a smart research assistant. Use the multiply_with_decorator tool to do multiplication. Use the divide_tool to do division. You can only make one tool call at a time. Do not call another tool until you have the output of the previous tool call. Example:\\nwhat is 2 times 3 divided by 4?\\nmutiply 2 and 3 first\\n2 times 3 is 6\\n\\n2 times 3 divided by 4 is \\n(2 times 3) divided by 4 which is\\n6 divided by 4\\ndivide 6 by 4 to get 1.5\\n\\nHuman: What is 11 times 142 divided by 8?\\nAI: \\nTool: 1562\\nTool: 195.25\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > llm:ChatOllama] [1.29s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"To find the result of 11 times 142 divided by 8, we first need to multiply 11 and 142.\\n\\nThe output of the multiplication is 1562.\\n\\nNext, we divide 1562 by 8.\\n\\nThe output of the division is 195.25.\",\n",
      "        \"generation_info\": {\n",
      "          \"model\": \"llama3.1:8b\",\n",
      "          \"created_at\": \"2025-03-08T09:18:38.536682Z\",\n",
      "          \"done\": true,\n",
      "          \"done_reason\": \"stop\",\n",
      "          \"total_duration\": 1288845125,\n",
      "          \"load_duration\": 12999292,\n",
      "          \"prompt_eval_count\": 251,\n",
      "          \"prompt_eval_duration\": 188000000,\n",
      "          \"eval_count\": 59,\n",
      "          \"eval_duration\": 1085000000,\n",
      "          \"message\": {\n",
      "            \"role\": \"assistant\",\n",
      "            \"content\": \"To find the result of 11 times 142 divided by 8, we first need to multiply 11 and 142.\\n\\nThe output of the multiplication is 1562.\\n\\nNext, we divide 1562 by 8.\\n\\nThe output of the division is 195.25.\",\n",
      "            \"images\": null,\n",
      "            \"tool_calls\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"To find the result of 11 times 142 divided by 8, we first need to multiply 11 and 142.\\n\\nThe output of the multiplication is 1562.\\n\\nNext, we divide 1562 by 8.\\n\\nThe output of the division is 195.25.\",\n",
      "            \"response_metadata\": {\n",
      "              \"model\": \"llama3.1:8b\",\n",
      "              \"created_at\": \"2025-03-08T09:18:38.536682Z\",\n",
      "              \"done\": true,\n",
      "              \"done_reason\": \"stop\",\n",
      "              \"total_duration\": 1288845125,\n",
      "              \"load_duration\": 12999292,\n",
      "              \"prompt_eval_count\": 251,\n",
      "              \"prompt_eval_duration\": 188000000,\n",
      "              \"eval_count\": 59,\n",
      "              \"eval_duration\": 1085000000,\n",
      "              \"message\": {\n",
      "                \"lc\": 1,\n",
      "                \"type\": \"not_implemented\",\n",
      "                \"id\": [\n",
      "                  \"ollama\",\n",
      "                  \"_types\",\n",
      "                  \"Message\"\n",
      "                ],\n",
      "                \"repr\": \"Message(role='assistant', content='To find the result of 11 times 142 divided by 8, we first need to multiply 11 and 142.\\\\n\\\\nThe output of the multiplication is 1562.\\\\n\\\\nNext, we divide 1562 by 8.\\\\n\\\\nThe output of the division is 195.25.', images=None, tool_calls=None)\"\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f16d7d48-c709-4e88-b760-037b036efc34-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 251,\n",
      "              \"output_tokens\": 59,\n",
      "              \"total_tokens\": 310\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > chain:ChannelWrite<...,llm>] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > chain:ChannelWrite<...,llm>] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > chain:action_exists] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm > chain:action_exists] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": false\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph > chain:llm] [1.29s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:LangGraph] [4.27s] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n"
     ]
    }
   ],
   "source": [
    "langchain.debug=True\n",
    "messages = [HumanMessage(content=\"What is 11 times 142 divided by 8?\")]\n",
    "# TODO make it use one tool at a time\n",
    "result = agent.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 11 times 142 divided by 8?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-03-08T09:14:14.810341Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2820774625, 'load_duration': 44469750, 'prompt_eval_count': 330, 'prompt_eval_duration': 1895000000, 'eval_count': 47, 'eval_duration': 877000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-33e8ea93-fb4a-44f4-915b-d8f56b9f56cd-0', tool_calls=[{'name': 'multiply_with_decorator', 'args': {'a': 11, 'b': 142}, 'id': '4375ed2b-dcbf-456c-b242-ced8695a9607', 'type': 'tool_call'}, {'name': 'divide', 'args': {'a': 1562, 'b': 8}, 'id': '81dd6019-0ad5-4fb1-8740-8e45c4894f9c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 330, 'output_tokens': 47, 'total_tokens': 377}),\n",
       " ToolMessage(content='1562', name='multiply_with_decorator', tool_call_id='4375ed2b-dcbf-456c-b242-ced8695a9607'),\n",
       " ToolMessage(content='195.25', name='divide', tool_call_id='81dd6019-0ad5-4fb1-8740-8e45c4894f9c'),\n",
       " AIMessage(content='The result of 11 times 142 divided by 8 is 1,562 with a remainder of 6.', additional_kwargs={}, response_metadata={'model': 'llama3.1:8b', 'created_at': '2025-03-08T09:14:15.461549Z', 'done': True, 'done_reason': 'stop', 'total_duration': 642651833, 'load_duration': 11890208, 'prompt_eval_count': 204, 'prompt_eval_duration': 188000000, 'eval_count': 25, 'eval_duration': 440000000, 'message': Message(role='assistant', content='The result of 11 times 142 divided by 8 is 1,562 with a remainder of 6.', images=None, tool_calls=None)}, id='run-89a0660e-d2d7-4555-bf5f-e58a11a07891-0', usage_metadata={'input_tokens': 204, 'output_tokens': 25, 'total_tokens': 229})]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['messages']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
